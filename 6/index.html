<!DOCTYPE html>
<html lang="en">

<head>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS 280A - Project 6: Made ya look!</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }

        .comparison-container {
            position: relative;
            overflow: hidden;
            cursor: ew-resize;
            margin: 40px auto;
            border: 2px solid #ddd;
        }

        .comparison-container img {
            position: absolute;
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        /* Mask for the left (A) image */
        .comparison-container .img-top {
            clip: rect(0, 300px, 400px, 0);
            /* Initial divider in the middle */
        }

        /* Semi-transparent background areas */
        .comparison-container .left-half,
        .comparison-container .right-half {
            position: absolute;
            top: 0;
            width: 50%;
            height: 100%;
            display: flex;
            justify-content: center;
            align-items: center;
            pointer-events: none;
            transition: opacity 0.5s ease;
        }

        .comparison-container .left-half {
            left: 0;
            background-color: rgba(149, 149, 149, 0.7);
            /* 50% opacity */
        }

        .comparison-container .right-half {
            right: 0;
            background-color: rgba(97, 97, 97, 0.424);
            /* 20% opacity */
        }

        /* Text styling for A and B labels */
        .comparison-container .label {
            color: white;
            font-size: 24px;
            font-weight: bold;
        }

        /* The vertical divider */
        .divider {
            position: absolute;
            top: 0;
            left: 50%;
            width: 3px;
            height: 100%;
            background-color: #ffffff00;
            cursor: ew-resize;
            transform: translateX(-50%);
            transition: background-color 0.5s ease;
        }

        /* On hover, hide the background areas with a smooth transition */
        .comparison-container:hover .left-half,
        .comparison-container:hover .right-half {
            opacity: 0;
        }

        /* On hover, the divider becomes more vibrant with smooth transition */
        .comparison-container:hover .divider {
            background-color: rgba(255, 255, 255, 0.347);
        }

        h1 {
            margin-top: 10px;
            margin-bottom: 10px;
        }

        h2 {
            color: #2c3e50;
            margin-top: 10px;
            margin-bottom: 20px;
        }

        h3 {
            color: #3f586f;
            margin-left: 40px;
            margin-top: 10px;
            margin-bottom: 10px;
            font-size: 24px;
        }

        ul.indented {
            margin-left: 20px;
        }

        p {
            margin-left: 20px;
        }

        p.indented {
            margin-left: 60px;
            /* Match or adjust the indent to be in line with your needs */
        }

        p.indented2,
        ol.indented2 {
            margin-left: 120px;
            /* Match or adjust the indent to be in line with your needs */
        }

        .highlight {
            font-weight: bold;
            color: rgb(70, 101, 138);
        }

        img {
            display: inline-block;
            /* Ensures the image itself is treated as a block element for centering */
            height: auto;
            max-width: 100%;
            /* Responsive behavior */
        }

        .pixelated {
            display: inline-block;
            /* Ensures the image itself is treated as a block element for centering */
            height: auto;
            max-width: 100%;
            image-rendering: pixelated;
            /* Ensures no interpolation for pixelated scaling */
            image-rendering: -moz-crisp-edges;
            /* Firefox */
            image-rendering: crisp-edges;
            /* Other browsers */
        }

        .displacement-matrix {
            text-align: center;
        }

        .image-wrapper {
            display: flex;
            justify-content: center;
            /* Center the image block horizontally */
            margin-bottom: 40px;
        }

        /* Modifier for different indentation */
        .image-wrapper.indent-level-1 {
            margin-left: 60px;
            margin-bottom: 0px;
        }

        .image-wrapper.indent-level-2 {
            margin-left: 80px;
        }

        .image-block {
            display: flex;
            /* Allows images and text to appear side by side */
            justify-content: center;
            /* Centers content horizontally */
            gap: 20px;
            /* Adds space between images */
        }

        .image-wrapper {
            text-align: center;
            /* Optional: Centers the entire block of images */
        }

        .image-item {
            display: flex;
            flex-direction: column;
            /* Makes text appear above the image */
            align-items: center;
            /* Centers the text and image horizontally */
        }

        .image-text {
            margin-bottom: 10px;
            /* Space between text and the image */
        }

        hr {
            border: 1px solid #ccc;
        }

        footer {
            text-align: center;
            margin-top: 50px;
            font-size: 0.9em;
            color: #777;
        }

        .grid-container {
            display: grid;
            grid-template-columns: auto 1fr 1fr 1fr 1fr;
            grid-template-rows: auto 1fr 1fr 1fr;
            gap: 10px;
            width: 800px;
            max-width: 100%;
            margin: 0 auto;
            padding-bottom: 40px;
        }

        .grid-container2 {
            display: grid;
            grid-template-columns: auto 1fr 1fr 1fr 1fr 1fr;
            grid-template-rows: auto 1fr 1fr;
            gap: 10px;
            width: 1200px;
            max-width: 100%;
            margin: 0 auto;
            padding-bottom: 40px;
        }

        .grid-container3 {
            display: grid;
            grid-template-columns: auto 1fr 1fr 1fr 1fr 1fr 1fr 1fr;
            grid-template-rows: auto 1fr 1fr 1fr;
            gap: 10px;
            width: 1200px;
            max-width: 100%;
            margin: 0 auto;
            padding-bottom: 40px;
        }

        .grid-container4 {
            display: grid;
            grid-template-columns: auto 1fr 1fr 1fr 1fr 1fr 1fr 1fr;
            grid-template-rows: auto 1fr 1fr;
            gap: 10px;
            width: 1200px;
            max-width: 100%;
            margin: 0 auto;
            padding-bottom: 40px;
        }

        .grid-container5 {
            display: grid;
            grid-template-columns: auto 1fr 1fr 1fr 1fr 1fr 1fr 1fr;
            grid-template-rows: auto 1fr;
            gap: 10px;
            width: 1200px;
            max-width: 100%;
            margin: 0 auto;
            padding-bottom: 40px;
        }

        .grid-container6 {
            display: grid;
            grid-template-columns: auto 1fr 1fr 1fr;
            grid-template-rows: auto 1fr 1fr 1fr 1fr;
            gap: 10px;
            width: 600px;
            margin: 0 auto;
            max-width: 100%;
            padding-bottom: 40px;
        }

        .grid-title {
            display: flex;
            align-items: center;
            justify-content: center;
            text-align: center;
            font-weight: bold;
        }


        .grid-image {
            width: 100%;
            object-fit: contain;
            /* Keeps images responsive */
            /* Optional: Additional styling for pixelated images */
        }
    </style>
</head>

<body>
    <h1>CS 280A Project 6: Made ya look!</h1>
    <h2>by Alonso Martinez and Tianshuang (Ethan) Qiu</h2>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/Title.png" alt="" width=800>
        </div>
    </div>
    <br><br>
    <h2>Intro</h2>


    <p class='indented'> <b>Where people look</b> is critical to visual communication
    </p>
    <br>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/OrderOfWherePeopleLook.gif" alt="" width=400>
        </div>
    </div>
    <br>
    <h2>Previous work</h2>
    <p class='indented'> Traditionally, techniques to influence where you look involve editing pixels by applying
        filters such as a blur or vignette to an image.
    </p>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/Vignette.png" alt="" width=900>
        </div>
    </div>
    <p class='indented'> However, people are incredible <b>fake discriminators</b> and we can spot that those pixels
        have been manipulated in a way that doesn't reflect the real world.
    </p>
    <br>
    <h2>Problem Statement</h2>
    <p class='indented'> In this project we explore creating generative models that influence where
        people look in an image to match the artist intent while keeping the final image inside the manifold of
        plausible realistic images.
    </p>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/Overview.png" alt="" width=900>
        </div>
    </div>
    <p class='indented'> We leverage existing state of the art deep learning models for gaze estimation
        (<a href="https://github.com/matthias-k/DeepGaze">DeepGaze</a>) and generative techniques such as <a
            href="https://sde-image-editing.github.io/">SDEdit</a> and <a
            href="https://github.com/andreas128/RePaint">RePaint</a> to create a Diffusion based architecture to induce
        the average viewer to look towards a mask painted by the user.
    </p>


    <br>
    <h2>Our Model</h2>
    <h3>Natural Vignette via SDEdit</h3>
    <p class='indented'> We apply the same vignette technique as mentioned before, but additionally we pass the image
        through SDEdit to ask, <b>"How could we achieve this look while keeping the image within the manifold of
            realistic
            images?"</b> <br><br>
        In the result below you can see that it added more <b>density to the foliage of the trees</b> in order to
        achieve a "<b><u>Natural vignette</b></u>" effect.
    </p>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/Title.png" alt="" width=900>
        </div>
    </div>
    <br><br>
    <h3>Content preservation via ReEdit and Text prompt</h3>
    <p class='indented'> As a design choice, we utilized the architecture proposed by <a
            href="https://github.com/andreas128/RePaint">RePaint</a> to give the artist the option to <b>preserve</b>
        the pixels in the area they want to draw attention (as specified by the target gaze map). To maintain context
        of the image, in the inverse of that mask, we use text prompt conditioning and CFG scale to allow for a
        re-imagining of the scene that might reduce distraction from the target gaze map.<br><br> People are one of the
        strongest gaze attractors in image. In the example below, you can see that the person in the yellow shirt is
        still present in the generated image, but they are much smaller and facing away from the camera (looking towards
        the temple).
    </p>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/RePaint.png" alt="" width=900>
        </div>
    </div>
    <br>
    <h3>Estimation of gaze</h3>
    <p class='indented'> Leveraging the state of the art model for gaze estimation <a
            href="https://github.com/matthias-k/DeepGaze">DeepGaze</a>, we can estimate where people will look in an
        image.
    </p>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/GazeEstimation.png" alt="" width=900>
        </div>
    </div>
    <br>
    <h2>Architecture</h2>
    <p class='indented'> Utilizing this gaze estimation, we can compute an L2 loss between the target gaze map and the
        target gaze map authored by the user. We experimented training a fine-tune layer on the diffusion model to
        utilize the gradient of the L2 loss, however all of the images in this project were generated by random sampling
        of the model and then using the L2 loss to sort the images that best match our goals.<br><br>

        Much like project 5, we utilize the pre-traiened diffusion model <a
            href="https://github.com/deep-floyd/IF">DeepFloyd</a> as the
        backbone of our generative model.
    </p>
    <br>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/Diagram.png" alt="" width=1024>
        </div>
    </div>
    <br>


    <h2>Results</h2>
    <h3>Results Legend</h3>
    <p class='indented'> The figure below explains how to interpret the results. The "Orig-Gen gazemap delta" (bottom
        right) is the result of subtracting the gaze map from the original image, from the gaze map of the generated
        image. It allows you to see how the edits affected DeepGaze's predictions of where people would look.
    </p>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/OutputGrid.jpg" alt="" width=768>
        </div>
    </div>
    <br>
    <h3>Results</h3>
    <p class='indented'> An interesting interpretation of the vignette as of *steam* on the corn. This is very different
        than effects traditionally achieved in Photoshop (pre-generative AI techniques).
    </p>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/002.jpg" alt="" width=1024>
        </div>
    </div>
    <br>
    <p class='indented'> The target gaze for this image is particularly interesting as the area of interest is spatially
        not centered and off to the right. In the orig-gen gaze delta (bottom right) one can see how the removal of the
        other helmets removed visual distractions.
    </p>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/003.jpg" alt="" width=1024>
        </div>
    </div>
    <br>
    <p class='indented'> A bokeh effect that is hallucinated which is cool given there is no equivalent filter in
        Photoshop that can achieve this.
    </p>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/004.jpg" alt="" width=1024>
        </div>
    </div>
    <br>
    <p class='indented'> The following <b>three</b> examples seem to change the person's gaze towards the object of
        interest (represented by the target gaze map).
    </p>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/010.jpg" alt="" width=1024>
        </div>
    </div>
    <br>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/007.jpg" alt="" width=1024>
        </div>
    </div>
    <br>
    <p class='indented'> We see an artifact of the hand becoming another dog.
    </p>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/006.jpg" alt="" width=1024>
        </div>
    </div>
    <br>
    <p class='indented'>The birds are removed from the image, the canoe color is simplified, a "natural vignette" is
        achieved with the clouds.
    </p>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/008.jpg" alt="" width=1024>
        </div>
    </div>
    <br>
    <p class='indented'>The area below the centipede is simplified and a blur and vignette effect is applied.
    </p>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/009.jpg" alt="" width=1024>
        </div>
    </div>
    <br>
    <p class='indented'>This one shows a dramatic interpretation of the pixels outside of the target gaze map. It makes
        the person in the yellow shirt smaller and facing away. Darkening the sky.
    </p>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/011.jpg" alt="" width=1024>
        </div>
    </div>
    <br>
    <p class='indented'>Another example of the same original image.
    </p>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/012.jpg" alt="" width=1024>
        </div>
    </div>
    <br>
    <p class='indented'>A simplification of the rocks below her.
    </p>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/001.jpg" alt="" width=1024>
        </div>
    </div>
    <br>
    <p class='indented'>While the main effect is that of blur, notice the background's color and shape complexity is
        simplified.
    </p>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <img src="./media/005.jpg" alt="" width=1024>
        </div>
    </div>
    <br>
    <h2>Future work</h2>
    <h3>Fine Tuning / LoRA</h3>
    <p class='indented'> As mentioned before, we intend to fine tune the model to follow the gradient of the L2 Loss to
        allow for more efficient sampling (for this project we did random sampling).
    </p>
    <h3>Human validation</h3>
    <p class='indented'> We have access to AR glasses (<a
            href="https://github.com/facebookresearch/projectaria_eyetracking?utm_source=chatgpt.com">Meta Aria</a>),
        which have eye-tracking capabilities. With this hardware we could validate the gaze map predictions of our
        generations.
    </p>
    <h3>Target gaze map automation</h3>
    <p class='indented'>We started to explore automating the target gaze map with simple mouse clicks on an image by
        leveraging state of the art monocular depth estimating models such as <a
            href="https://huggingface.co/docs/diffusers/en/using-diffusers/marigold_usage">Marigold</a>. Below we show
        how based on a mouse click we could manipulate the depth information to be object centric rather than camera
        centric:
    </p>
    <div class="image-wrapper indent-level-1">
        <div class="image-block">
            <div class="image-item">
                <div class="image-text" style="font-weight: bold;">Marigold's depth estimation</div>
                <img src="./media/Depth_orig.jpg" alt="" width="512">
            </div>
            <div class="image-item">
                <div class="image-text" style="font-weight: bold;">Marigold's camera centric depth estimation</div>
                <img src="./media/Depth_marigold.png" alt="" width="512">
            </div>
            <div class="image-item">
                <div class="image-text" style="font-weight: bold;">Object centric depth estimation</div>
                <img src="./media/Depth_obj_centric.png" alt="" width="512">
            </div>
        </div>
    </div>
    <h3>Hybrid Vignette</h3>
    <p class='indented'>We wanted to try to apply the vignette to the lower frequencies of the image and preserve the
        high frequencies. This would allow for a more natural looking vignette effect.
    </p>
    <h3>RePaint masks improvements</h3>
    <p class='indented'> If you subtract the target gaze map from the original image's gaze map prediction you
        essentially get a mask of the distracting areas. We want to explore using this as a mask for RePaint.
    </p>
    <h3>Gaze order sequence</h3>
    <p class='indented'> For storytelling purposes, the ability to induce gaze order sequence would be a powerful tool!
    </p>
    <h3>Pyramid gaze manipulation</h3>
    <p class='indented'> In art, it's often useful to <b>thumbnail</b> a composition of an image before creating the
        full
        image. Intuitively you can imagine how computing gaze manipulation from our model at multiple resolutions might
        improve our results.
    </p>
    <br>
    <h2>Final thoughts</h2>
    <p class=indent>
        As you can tell by the long list of items in our future work section, this class has <b>filled us with
            passion</b> for exploring the possibilities of computer vision, generative models, and creating beautiful
        art. Thank you so much for everything!!!
    </p>
    <footer>
        CS 280A - Intro to Computer Vision | Alonso Martinez and Tianshuang (Ethan) Qiu
    </footer>
    <script>
        function setLabelsAndSize(containerId) {
            const container = document.getElementById(containerId);
            const leftLabel = document.getElementById('leftLabel');
            const rightLabel = document.getElementById('rightLabel');
            const imgA = document.getElementById('imgA');
            const imgB = document.getElementById('imgB');

            // Get the titles from the data attributes
            const leftTitle = container.getAttribute('data-left-title');
            const rightTitle = container.getAttribute('data-right-title');

            // Update the labels
            leftLabel.textContent = leftTitle;
            rightLabel.textContent = rightTitle;

            // Get width and height from data attributes
            let width = container.getAttribute('data-width');
            let height = container.getAttribute('data-height');

            // Get the original aspect ratio from the first image
            const aspectRatio = imgA.naturalWidth / imgA.naturalHeight;

            // If only width is provided, calculate the height based on the aspect ratio
            if (width && !height) {
                height = width / aspectRatio;
            }

            // If only height is provided, calculate the width based on the aspect ratio
            if (!width && height) {
                width = height * aspectRatio;
            }

            // Apply the width and height to the container
            container.style.width = `${width}px`;
            container.style.height = `${height}px`;
        }

        // Call the function to set custom titles and sizes
        window.onload = function () {
            setLabelsAndSize('comparisonContainer');
        };

        // Add mousemove functionality for the comparison
        const comparisonContainer = document.getElementById('comparisonContainer');
        const divider = document.getElementById('divider');
        const imgTop = document.querySelector('.img-top');

        comparisonContainer.addEventListener('mousemove', (event) => {
            // Get the position of the mouse relative to the container
            const rect = comparisonContainer.getBoundingClientRect();
            const x = event.clientX - rect.left; // x position of the mouse inside the container

            // Set the position of the divider
            divider.style.left = x + 'px';

            // Clip the top image to create the A/B effect
            imgTop.style.clip = `rect(0, ${x}px, 400px, 0)`;
        });

        comparisonContainer.addEventListener('mouseleave', () => {
            // Reset the divider to the middle when the mouse leaves the container
            const middle = comparisonContainer.offsetWidth / 2;
            divider.style.left = middle + 'px';
            imgTop.style.clip = `rect(0, ${middle}px, 400px, 0)`;
        });
    </script>

</body>

</html>